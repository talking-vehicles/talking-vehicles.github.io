<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Talking Vehicles: Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Talking Vehicles: Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play</title>

  <!-- TODO: Global site tag (gtag.js) - Google Analytics -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Talking Vehicles: Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play</h1>
          <div class="is-size-5 publication-authors">
      <span class="author-block">
      <a href="https://cuijiaxun.github.io/">Jiaxun Cui</a><sup>1</sup></a>,
      <a href="https://chentangmark.github.io/">Chen Tang</a><sup>1</sup>,
      Jarrett Holtz<sup>2</sup>,
      Janice Nguyen<sup>3</sup>,
      <a href="https://scholar.google.co.uk/citations?user=T5JSHMoAAAAJ&hl=en">Alessandro G. Allievi</a><sup>2</sup>,
      <a href="https://hangqiu.github.io/">Hang Qiu</a><sup>3</sup>,
      <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a><sup>1,4</sup>
      </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Texas at Austin</span>,
            <span class="author-block"><sup>2</sup>Robert Bosch LLC</span>,
            <span class="author-block"><sup>3</sup>University of California, Riverside</span>,
            <span class="author-block"><sup>4</sup>Sony AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/pdf/Talking_Vehicles__CoRL_2025_Arxiv.pdf"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2401.12963"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/cuijiaxun/talking-vehicles"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://utexas.box.com/s/5pn2rfcprqv9p992m9lzyeaurq8irrhd"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-play"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
              <!-- BibTex Link. -->
              <span class="link-block">
                <a href="#BibTeX"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-book-open"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Past work has demonstrated that autonomous vehicles can drive more safely if they communicate with one another than if they do not. 
            However, their communication has often not been human-understandable. Using natural language as a vehicle-to-vehicle (V2V) 
            communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with 
            human drivers. In this work, we propose a suite of traffic tasks in autonomous driving where vehicles in a traffic scenario need 
            to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient 
            traffic flow. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and high-level decision-making 
            policy for autonomous vehicles through multi-agent discussion. To evaluate LLM agents for driving, we developed a gym-like simulation 
            environment that contains a range of driving scenarios. Our experimental results demonstrate that LLM+Debrief is more effective at 
            generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than a zero-shot 
            LLM agent. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!--Talking Vehicles Gym Environment-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">TalkingVehiclesGym</h2>
        <div class="content has-text-justified">
          <p>
            TalkingVehiclesGym is a multi-agent gymnasium simulation environment for the closed-loop evaluation of urban driving policies. 
            It is designed to evaluate the performance of multi-agent communication and collaboration for an array of cooperative driving tasks. 
            TalkingVehiclesGym supports in-episode communication in among autonomous agents through <a href="https://mqtt.org/">MQTT</a>.
          </p>
          <p>
            <img style="max-width:100%" src="./static/images/talking_vehicle_scenarios.png" class="img-reponsive" />
            <figcaption class="has-text-centered has-text-grey-dark is-size-8">
              <b><strong>Figure 1.</strong>Overview of Scenarios and Agent Roles.</b>
              <span style="color: #006400;" class="has-text-weight-bold">Green circles</span>: Focal agents (establish coordination through communication); 
              <span style="color: #8B0000;" class="has-text-weight-bold">Red circles</span>: Potential colliders (violate traffic rules or do not yield); 
              <span style="color: #00008B;" class="has-text-weight-bold">Blue circles</span>: Background agents (not necessarily adversarial).
            </figcaption>
          </p>
        </div>
      </div>
    </div>
</section>

<!--LLM+Debrief Method-->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">
          LLM+Debrief
          <span class="is-size-4" style="color: #666666;">Learn to Collaborate using Natural Language via Self-Play</span>
        </h2>
        <p>
          We ask the key research question that whether the current LLM agents are able to coordination well in the driving task without pre-coordination,
          and whether their cooperation capability could be improved over interactions. We start from building an agent that only rely on Chain-of-Thought,
          gradually extending to self-reflections and centralized discussions (LLM+Debrief) to create knowledge for future driving and collaborations.
        </p>
        <br>
        <div class="content has-text-justified">
          <p style="text-align:center;">
            <img style="max-width:100%" src="./static/images/llm_debrief_method.png" class="img-reponsive" />
            <figcaption class="has-text-centered has-text-grey-dark is-size-8">
              <b><strong>Figure 2.</strong>LLM+Debrief Agent Framework and Agent Learning Pipeline.</b>
            </figcaption>
          </p>
        </div>
        <div class="content has-text-justified">
          <p class="has-text-weight-bold is-size-4 has-text-grey-dark">Agent Policy</p>
            <p>
              The agent's probability distribution among actions is decided by an LLM with a prompt that contains the observations, tasks, traffic rules,
              received messages from other vehicles, and other important information for driving. The agent output must be in the json format that decides 
              a command and/or a message to send.
            </p>
          <ol>
            <li>
              <strong>In-Context Knowledge</strong>
              <p>
                Since the language models are auto-regressive, the probability of taking action is affected by the context provided to the agents.
                Thus, we can augment the prompt with the prior knowledge to alter the policy.
              </p>
            </li>
            <li>
              <strong>Chain-of-Thought Reasoning</strong>
              <p>
                We prompt the agent to reason about its surrounding and possible consequence of each action first, then then serve the output reasoning 
                as a part of the prompt for final decisions.
              </p>
            </li>
          </ol>
        
          <p class="has-text-weight-bold is-size-4 has-text-grey-dark">Agent Learning</p>
            <p>
              Initially, the LLM agents interact with each other in the scenarios and store their experience in the relay buffer. The agents then
              enage in a discussion session where they sample past experience as a context to refine their joint cooperative strategy.
            </p>
          <ol>
            <li>
              <strong>Replay Buffer</strong>
                <p>
                  We store the transition data (observations, actions, next observations) in a replay buffer. When an episode concludes, the environment will
                  each agent's performance and provide scalar rewards, as well as verbal feedbacks like "Vehicle 109 collided with Vehicle 110 after 2 seconds"
                  or "Vehicle 111 stagnated for too long to complete its task." Each transition data is retrospectively labeled with enriched meta data, including 
                  responses from other agents, collision details, stagnaition specifics and final outcomes.
                </p>
            </li>
            <li>
              <strong>Batch Sampling</strong>
              <p>
                While analyzing the entire trajectory would provide
                a comprehensive understanding of failure cases, computational constraints necessitate sampling a
                subset (batch) of keyframes from its replay buffer. To prioritize relevant data, the sampling process
                heuristically assigns higher probabilities to transitions that occur immediately
                before collisions, involve actions contributing to collisions, or lead to stagnation due to agents slowing
                down. Additionally, transitions that feature more intensive multi-agent interactions are given more
                weight.
              </p>
            </li>
            <li>
              <strong>Debrief</strong>
              <p>
                A debriefing session begins when an episode concludes in failure (collision or stagnation) 
                and is conducted in a turn-based manner over N rounds, with a focus on improving cooperation
                in future interactions. The speaking order is deterministic in this work for each session, and agents
                take turns speaking in a round-robin format. The agent chosen to speak first is responsible for
                proposing a joint cooperative strategy for everyone participating in the debriefing
                (the focal group). This agent begins by reasoning through its transition data batch, analyzing the
                consequences and influence on other agents of its actions, and formulating a proposed strategy.
                Subsequently, the other agents take turns sharing their perspectives, providing feedback, or offering
                alternative insights based on their analysis of their own experience batch. After the discussion, each
                agent summarizes the discussion to develop individual cooperative strategies and knowledge. 
                These outcomes serve as in-context guidelines for future driving tasks. This joint discussion
                for future individual decision-making structure mirrors the principles of the Centralized Training
                Decentralized Execution (CTDE) framework.
              </p>
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<!--Experiments-->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Experiments</h2>
    <p>We established several baselines and scenarios to answer the research questions:</p>
    <ul>
      <li>
        <strong>Zero-shot</strong>: an LLM agent using Chain-of-Thought (CoT) reasoning only.
      </li>
      <li>
        <strong>Reflection</strong>: an LLM agent with CoT reasoning contextualized with knowledge from decentralized reflection.
      </li>
      <li>
        <strong>Correction+RAG</strong>: an LLM agent that corrects past actions via self-reflection, 
        stores these corrections in a vector-based retrievable memory, and uses few-shot retrieved-example augmented generation.
      </li>
      <li>
        <strong>Correction+RAG (Silent)</strong>: the retrieval-augmented method without communication (adapting <a href="https://pjlab-adg.github.io/DiLu/">DiLU</a>, 
        a non-communicating single-agent LLM-based reflection-driving approach, to our environment).
      </li>
      <li>
        <strong>Correction+RAG (Comm)</strong>: the multi-agent communication extension of DiLU (<a href="https://arxiv.org/abs/2404.06345">AgentsCoDriver</a>), 
        which resembles Correction+RAG but does not actively optimize the messages.
      </li>
    </ul>
    <p>
      For a fair comparison across these LLM-based baselines, we did not initialize any knowledge with human data, nor was there human involvement during the learning process.
    </p>
    <figure class="image is-full-width" style="margin-top: 2em;">
      <img src="./static/images/talking_vehicle_quantative_results.png" alt="Quantitative results for the talking vehicles experiments">
      <figcaption class="has-text-centered">
        <strong>Figure 3.</strong> Quantitative performance comparison across baselines: Zero-shot, Reflection, Correction+RAG, and their Communication/Silent variants.
      </figcaption>
    </figure>
  </div>
</section>


<!-- Qualitative Videos -->
<section class="section" id="QualitativeVideos">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Qualitative Videos</h2>
    
    <!-- Row 1 -->
    <div class="columns">
      <div class="column">
        <h4 class="subtitle is-5" style="color: #666666;">Scenario 1: Perception Overtake</h4>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/perception_overtake_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="column">
        <h4 class="subtitle is-5" style="color: #666666;">Scenario 2: Perception Red Light</h4>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/perception_red_light_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

    <!-- Row 2 -->
    <div class="columns">
      <div class="column">
        <h4 class="subtitle is-5" style="color: #666666;">Scenario 3: Perception Left Turn</h4>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/perception_left_turn_risky_debrief_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="column">
        <h4 class="subtitle is-5" style="color: #666666;">Scenario 4: Negotiation Overtake</h4>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/negotiation_overtake_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

    <!-- Row 3 -->
    <div class="columns">
      <div class="column">
        <h4 class="subtitle is-5" style="color: #666666;">Scenario 5: Negotiation Highway Merge</h4>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/negotiation_highway_merge_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="column">
        <h4 class="subtitle is-5" style="color: #666666;">Scenario 6: Negotiation Highway Exit</h4>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/negotiation_highway_exit_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

  </div>
</section>

<!-- Example Learned Knowledge -->
<section class="section" id="example-learned-knowledge">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Example Learned Knowledge</h2>

    <!-- Tabs navigation -->
    <div class="tabs is-toggle" id="scenario-tabs">
      <ul>
        <li class="is-active" data-tab="perception-overtake"><a>Perception Overtake</a></li>
        <li data-tab="perception-red-light"><a>Perception Red Light</a></li>
        <li data-tab="perception-left-turn"><a>Perception Left Turn</a></li>
        <li data-tab="negotiation-overtake"><a>Negotiation Overtake</a></li>
        <li data-tab="negotiation-highway-merge"><a>Negotiation Hwy Merge</a></li>
        <li data-tab="negotiation-highway-exit"><a>Negotiation Hwy Exit</a></li>
      </ul>
    </div>

    <!-- Content panes -->
    <div>
      <div id="perception-overtake" class="tab-content">
        <h3 class="title is-4">Perception Overtake</h3>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/perception_overtake_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p><strong>Car1 (Overtaking)</strong></p>
        <p>
          In our refined cooperative strategy, the stationary vehicle in lane 1 will continuously monitor lane -1 
          for any approaching traffic and provide real-time alerts. Meanwhile, the overtaking vehicle will actively 
          use its sensors to independently verify the status of lane -1. If both the stationary vehicle's observations 
          and the overtaking vehicle's sensors confirm that lane -1 is clear, the overtaking vehicle will promptly 
          proceed with the maneuver to avoid stagnation. If the stationary vehicle detects any traffic in lane -1, 
          it will immediately communicate this, advising the overtaking vehicle to hold its position. The overtaking 
          vehicle will remain ready to accelerate smoothly once the lane is confirmed clear. This dual-verification approach, 
          combining sensor data and real-time communication, ensures a timely and safe overtaking maneuver, 
          preventing collisions and minimizing stagnation.
        </p>
      </div>
      <div id="perception-red-light" class="tab-content is-hidden">
        <h3 class="title is-4">Perception Red Light</h3>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/perception_red_light_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>…learned knowledge for Perception Red Light…</p>
      </div>
      <div id="perception-left-turn" class="tab-content is-hidden">
        <h3 class="title is-4">Perception Left Turn</h3>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/perception_left_turn_risky_debrief_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>…learned knowledge for Perception Left Turn…</p>
      </div>
      <div id="negotiation-overtake" class="tab-content is-hidden">
        <h3 class="title is-4">Negotiation Overtake</h3>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/negotiation_overtake_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>…learned knowledge for Negotiation Overtake…</p>
      </div>
      <div id="negotiation-highway-merge" class="tab-content is-hidden">
        <h3 class="title is-4">Negotiation Highway Merge</h3>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/negotiation_highway_merge_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>…learned knowledge for Negotiation Hwy Merge…</p>
      </div>
      <div id="negotiation-highway-exit" class="tab-content is-hidden">
        <h3 class="title is-4">Negotiation Highway Exit</h3>
        <video controls style="width:100%; height:auto; border:1px solid #ddd;">
          <source src="./static/videos/negotiation_highway_exit_risky_distill_success.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>…learned knowledge for Negotiation Hwy Exit…</p>
      </div>
    </div>
  </div>
</section>

<!-- Wrap-tabs CSS -->
<style>
  #scenario-tabs ul {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
  }
  #scenario-tabs ul li {
    flex: 0 1 auto;
    margin: 0.25rem;
  }
</style>

<!-- Tab-switching script -->
<script>
  document.getElementById('scenario-tabs').addEventListener('click', function(e) {
    const li = e.target.closest('li[data-tab]');
    if (!li) return;
    this.querySelectorAll('li').forEach(tab => tab.classList.remove('is-active'));
    document.querySelectorAll('#example-learned-knowledge .tab-content').forEach(c => c.classList.add('is-hidden'));
    li.classList.add('is-active');
    document.getElementById(li.dataset.tab).classList.remove('is-hidden');
  });
</script>




<!--Distillation-->
<!-- Distillation -->
<section class="section" id="distillation">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Distillation and Generalization</h2>
    <p>
      We perform full-parameter fine-tuning of a compact language model (DistilGPT2&nbsp;<a href="https://arxiv.org/abs/1910.01108">Sanh et al., 2019</a>) 
      to directly <strong>imitate</strong> the behavior of our large, memory-augmented LLM+Debrief agent. To build the imitation dataset, 
      we collect every successful evaluation episode across all six scenarios and record the large agent’s token-level outputs. The distillation model 
      is then trained to minimize cross-entropy loss against those outputs. At inference time, it generates decisions via random sampling with a 
      temperature of 0.2.
    </p>
  </div>
  <div class="container is-max-desktop content">
    <p class="subtitle is-6">
      Each policy is evaluated with three random seeds (30 episodes per seed). We report the mean and ±1 SEM across seeds. 
      <span class="has-text-grey">Debrief (per-scenario)</span> is an oracle baseline learned individually per scenario.
    </p>

    <table class="table is-fullwidth is-striped is-hoverable">
      <thead>
        <tr>
          <th rowspan="2">Method</th>
          <th colspan="2">Overtake (Perception)</th>
          <th colspan="2">Red Light</th>
          <th colspan="2">Left Turn</th>
        </tr>
        <tr>
          <th>CR (%) ↓</th>
          <th>SR (%) ↑</th>
          <th>CR (%) ↓</th>
          <th>SR (%) ↑</th>
          <th>CR (%) ↓</th>
          <th>SR (%) ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="has-text-grey">Debrief (per-scenario)</td>
          <td class="has-text-grey">1.1 ± 1.1</td>
          <td class="has-text-grey"><strong>98.9 ± 1.1</strong></td>
          <td class="has-text-grey">0.0 ± 0.0</td>
          <td class="has-text-grey">96.7 ± 0.0</td>
          <td class="has-text-grey">4.4 ± 2.9</td>
          <td class="has-text-grey">94.4 ± 2.2</td>
        </tr>
        <tr>
          <td>Centralized Memory</td>
          <td>2.2 ± 1.1</td>
          <td>93.3 ± 1.9</td>
          <td>0.0 ± 0.0</td>
          <td><strong>100.0 ± 0.0</strong></td>
          <td>4.4 ± 2.9</td>
          <td>93.3 ± 3.3</td>
        </tr>
        <tr>
          <td>Distillation</td>
          <td><strong>0.0 ± 0.0</strong></td>
          <td>83.3 ± 1.9</td>
          <td>0.0 ± 0.0</td>
          <td>91.1 ± 4.4</td>
          <td><strong>0.0 ± 0.0</strong></td>
          <td><strong>96.7 ± 0.0</strong></td>
        </tr>
      </tbody>

      <thead>
        <tr>
          <th rowspan="2">Method</th>
          <th colspan="2">Overtake (Negotiation)</th>
          <th colspan="2">Highway Merge</th>
          <th colspan="2">Highway Exit</th>
        </tr>
        <tr>
          <th>CR (%) ↓</th>
          <th>SR (%) ↑</th>
          <th>CR (%) ↓</th>
          <th>SR (%) ↑</th>
          <th>CR (%) ↓</th>
          <th>SR (%) ↑</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="has-text-grey">Debrief (per-scenario)</td>
          <td class="has-text-grey">10.0 ± 3.8</td>
          <td class="has-text-grey">87.2 ± 3.9</td>
          <td class="has-text-grey">2.2 ± 2.2</td>
          <td class="has-text-grey">97.8 ± 2.2</td>
          <td class="has-text-grey">13.3 ± 6.0</td>
          <td class="has-text-grey">86.7 ± 6.0</td>
        </tr>
        <tr>
          <td>Centralized Memory</td>
          <td>12.2 ± 2.9</td>
          <td>86.7 ± 1.9</td>
          <td>1.1 ± 1.1</td>
          <td>98.9 ± 1.1</td>
          <td>16.1 ± 4.8</td>
          <td>82.8 ± 5.3</td>
        </tr>
        <tr>
          <td>Distillation</td>
          <td><strong>10.0 ± 3.3</strong></td>
          <td><strong>88.9 ± 4.4</strong></td>
          <td><strong>0.0 ± 0.0</strong></td>
          <td><strong>100.0 ± 0.0</strong></td>
          <td><strong>3.3 ± 0.0</strong></td>
          <td><strong>96.7 ± 0.0</strong></td>
        </tr>
      </tbody>
    </table>
  </div>
</section>

<div class="container is-max-desktop content">
  <h2 class="title is-4">Decision Latency &amp; Message Size (Distilled LLM Policy)</h2>

  <table class="table is-fullwidth is-striped is-hoverable">
    <caption class="is-sr-only">
      Decision Latency and Message Size using Distilled LLM Policy across scenarios
    </caption>
    <thead>
      <tr>
        <th>Latency / Scenario</th>
        <th>Overtake</th>
        <th>Left Turn</th>
        <th>Red Light</th>
        <th>Overtake</th>
        <th>Highway Merge</th>
        <th>Highway Exit</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Decision Latency (s)</strong></td>
        <td>0.45</td>
        <td>0.44</td>
        <td>0.38</td>
        <td>0.14</td>
        <td>0.19</td>
        <td>0.20</td>
      </tr>
      <tr>
        <td><strong>Message Size (bytes)</strong></td>
        <td>223.3</td>
        <td>297.9</td>
        <td>223.0</td>
        <td>28.0</td>
        <td>59.0</td>
        <td>59.0</td>
      </tr>
    </tbody>
  </table>
</div>


<!-- Future Work -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Future Work</h2>
    <p>
      Talking Vehicles is a promising step towards AI agents that can purposefully communicate with each other 
      and humans in a natural language to coordinate on a task via self-play among agents. Future work will be
      directed towards creating more robust and diverse learned policies to enable ad hoc teamwork, 
      integrating larger multimodal models, and studying the learning methods that can train the agents to perform more complex tasks.
    </p>
  </div>
</section>

<!-- BibTex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{cui2025talking,
      title={Talking Vehicles: Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play}, 
      author={Jiaxun Cui and Chen Tang and Jarrett Holtz and Janice Nguyen and Alessandro G. Allievi and Hang Qiu and Peter Stone},
      year={2025},
      eprint={TODO},
      archivePrefix={arXiv},
      primaryClass={TODO}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         target="_blank"
         href="./static/pdf/Talking_Vehicles__CoRL_2025_Arxiv.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>This website is borrowed from <a href="https://nerfies.github.io/">nerfies</a>.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
